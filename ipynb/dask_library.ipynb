{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook full of useful codes and libraries for data science and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting pandas df limits\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json to dataframe\n",
    "\n",
    "with open('Assessor-Search-Results.json', 'r', encoding=\"utf-8\") as json_file:\n",
    "    json_work = json.load(json_file)\n",
    "\n",
    "df = pd.json_normalize(json_work)\n",
    "\n",
    "df = pd.DataFrame([y for x in df['results'].values.tolist() for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MSSQL Connection, import from SQL to pandas dataframe\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "config = dict(server=   'SERVER',\n",
    "              port=      PORT,\n",
    "              database= 'DATABASE',\n",
    "              username= 'USERNAME',\n",
    "              password= 'PASSWORD')\n",
    "\n",
    "cxn_str = ('SERVER={server},{port};' + 'DATABASE={database};' +\n",
    "           'UID={username};' + 'PWD={password}')\n",
    "\n",
    "cxn = pyodbc.connect(r'DRIVER={SQL Server Native Client 11.0};' + cxn_str.format(**config))\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM TABLE;\", cxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" \n",
    "    iterate through all the columns of a dataframe and \n",
    "    modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage of dataframe is {:.2f}' \n",
    "                     'MB').format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max <\\\n",
    "                  np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max <\\\n",
    "                   np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max <\\\n",
    "                   np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max <\\\n",
    "                   np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max <\\\n",
    "                   np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max <\\\n",
    "                   np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage after optimization is: {:.2f}' \n",
    "                              'MB').format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \n",
    "                                             / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data file incrementally using pandas, you have to use a parameter chunksize which specifies number of rows to read/write at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_dataframe = pd.read_csv(\n",
    "    \"train.csv\", chunksize=100000)  # Number of lines to read.\n",
    "# This method will return a sequential file reader (TextFileReader)\n",
    "# reading 'chunksize' lines every time. To read file from\n",
    "# starting again, you will have to call this method again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can train on your data incrementally using XGBoost¹ or LightGBM. For LightGBM you have to pass in a argument keep_training_booster=True to its .train method and three arguments to XGBoost's .train method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First one necessary for incremental learning:\n",
    "lgb_params = {\n",
    "    'keep_training_booster': True,\n",
    "    'objective': 'regression',\n",
    "    'verbosity': 100,\n",
    "}\n",
    "# First three are for incremental learning:\n",
    "xgb_params = {\n",
    "    'update': 'refresh',\n",
    "    'process_type': 'update',\n",
    "    'refresh_leaf': True,\n",
    "    'silent': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each step we will save our estimator and then pass it as an argument during next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving regressor for next use.\n",
    "lgb_estimator = None\n",
    "xgb_estimator = None\n",
    "\n",
    "for df in incremental_dataframe:\n",
    "    df = preprocess(df)\n",
    "  \n",
    "    xtrain, ytrain, xvalid, yvalid = # Split data as you like\n",
    "  \n",
    "    lgb_estimator = lgb.train(lgb_params,\n",
    "                         # Pass partially trained model:\n",
    "                         init_model=lgb_estimator,\n",
    "                         train_set=lgb.Dataset(xtrain, ytrain),\n",
    "                         valid_sets=lgb.Dataset(xvalid, yvalid),\n",
    "                         num_boost_round=10)\n",
    "\n",
    "    xgb_model = xgb.train(xgb_params, \n",
    "                        dtrain=xgb.DMatrix(xtrain, ytrain),\n",
    "                        evals=(xgb.DMatrix(xvalid, yvalid),\"Valid\"),\n",
    "                        # Pass partially trained model:\n",
    "                        xgb_model = xgb_estimator)\n",
    "\n",
    "    del df, xtrain, ytrain, xvalid, yvalid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost's incremental learning method is in progress.²\n",
    "To speed things up a bit more and if your chunks a still sufficiently big, you can parallelize your preprocessing method using Python's multiprocessing library functions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 4\n",
    "for df in incremental_dataframe:\n",
    "    p = Pool(n_jobs)\n",
    "    f_ = p.map(preprocess, np.array_split(df, n_jobs))\n",
    "    f_ = pd.concat(f_, axis=0, ignore_index=True)\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "    # And then your model training ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an introduction on Parallel programming in Python read my post here.\n",
    "\n",
    "https://towardsdatascience.com/speed-up-your-algorithms-part-1-pytorch-56d8a4ae7051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
